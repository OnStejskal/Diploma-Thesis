{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.dataset import SegmentationDataset\n",
    "from os.path import join\n",
    "from common.visualization import plot_image_label\n",
    "from common.transformations import (\n",
    "    SegCompose,\n",
    "    SegCrop,\n",
    "    SegRandomRotation,\n",
    "    SegRandomVerticalFlip,\n",
    "    SegRandomHorizontalFlip,\n",
    "    SegRandomRotation,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    CenterCrop,\n",
    ")\n",
    "IMG_MAX_SIZE = (400,400)\n",
    "TRAIN_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        # SegRandomRotation(0.99),\n",
    "        # SegCrop(default_t=5, default_size = (IMG_MAX_SIZE)),\n",
    "         SegCrop(default_t=15),\n",
    "    ]\n",
    ")\n",
    "VAL_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=5, default_size = (IMG_MAX_SIZE)),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "TRANSFORMATIONS_TORCH= Compose(\n",
    "    [\n",
    "        Resize((256,256)),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "# VAL_TRANSFORMATIONS_SEG2 = SegCompose(\n",
    "#     [\n",
    "#         SegCrop(default_t=15, default_size=IMG_MAX_SIZE),\n",
    "#     ]\n",
    "# )\n",
    "# dst = SegmentationDataset(join(\"data\", \"cropped_fully_annotated\", \"train\", \"images\"), join(\"data\", \"cropped_fully_annotated\", \"train\", \"segmentations\"), TRAIN_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "# dsv = SegmentationDataset(join(\"data\", \"cropped_fully_annotated\", \"train\", \"images\"), join(\"data\", \"cropped_fully_annotated\", \"train\", \"segmentations\"), VAL_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "dst =  SegmentationDataset(join(\"data\", \"train_val_test\", \"train\", \"images\"), join(\"data\", \"train_val_test\", \"train\", \"segmentations\"), TRAIN_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "dsv = SegmentationDataset(join(\"data\", \"train_val_test\", \"train\", \"images\"), join(\"data\", \"train_val_test\", \"train\", \"segmentations\"), VAL_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "weak = SegmentationDataset(join(\"data\", \"weak_annotations\", \"images\"), join(\"data\", \"weak_annotations\", \"segmentations\"), TRAIN_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "for i in range(min(20,len(weak))):\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    # (img, label) = dst[i]\n",
    "    # print(img.shape)\n",
    "    # print(label.shape)\n",
    "    # print(img)\n",
    "    # plot_image_label(img, label)\n",
    "   \n",
    "    # (img, label) = dsv[i]\n",
    "    # print(img.shape)\n",
    "    # print(label.shape)\n",
    "    # print(img)\n",
    "    # plot_image_label(img, label)\n",
    "    # break\n",
    "    (img, label) = dst[i]\n",
    "    plot_image_label(img, label)\n",
    "    \n",
    "    (img, label) = dsv[i]\n",
    "    plot_image_label(img, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"data/cropped_fully_annotated/test/images/s_201504280920160358VAS.png\")\n",
    "color_mode = img.mode\n",
    "\n",
    "# Print the number of channels\n",
    "num_channels = len(color_mode)\n",
    "print(f\"Number of channels: {num_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"data/train_val_test/test/images/s_201504280920160358VAS.png\")\n",
    "color_mode = img.mode\n",
    "\n",
    "# Print the number of channels\n",
    "num_channels = len(color_mode)\n",
    "print(f\"Number of channels: {num_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def randomly_swap_classes(segmentation, swap_prob=0.1):\n",
    "    unique_classes = np.unique(segmentation)\n",
    "    print(unique_classes)\n",
    "    swapped_segmentation = np.copy(segmentation)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        # Create a mask for the current class\n",
    "        mask = (segmentation == cls)\n",
    "\n",
    "        # Calculate the number of swaps needed\n",
    "        num_swaps = int(np.sum(mask) * swap_prob)\n",
    "\n",
    "        # Choose random pixels to swap\n",
    "        swap_indices = np.random.choice(np.flatnonzero(mask), size=num_swaps, replace=False)\n",
    "\n",
    "        # Choose random classes to swap with\n",
    "        swap_with_classes = np.random.choice(unique_classes[unique_classes != cls], size=num_swaps)\n",
    "\n",
    "        # Perform the swap\n",
    "        np.put(swapped_segmentation, swap_indices, swap_with_classes)\n",
    "\n",
    "    return swapped_segmentation\n",
    "\n",
    "# Example usage\n",
    "label_segmentation = np.array([[1, 2, 2, 3], [1, 1, 2, 3], [3, 3, 1, 2], [2, 3, 3, 1]])\n",
    "swapped_segmentation = randomly_swap_classes(label_segmentation)\n",
    "print(\"Original Segmentation:\\n\", label_segmentation)\n",
    "print(\"Swapped Segmentation:\\n\", swapped_segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from \n",
    "VAL_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=15, square_output = True),\n",
    "    ]\n",
    ")\n",
    "IMG_SHAPE = (256, 256)\n",
    "TRANSFORMATIONS_TORCH = Compose(\n",
    "    [\n",
    "        # CenterCrop(IMG_MAX_SIZE),\n",
    "        Resize(IMG_SHAPE),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def label_swap_pixels(segmentation, swap_prob=0.1):\n",
    "    unique_classes = np.unique(segmentation)\n",
    "    flattened_segmentation = segmentation.flatten()\n",
    "\n",
    "    # Calculate the number of pixels to swap\n",
    "    num_pixels_to_swap = int(len(flattened_segmentation) * swap_prob)\n",
    "\n",
    "    # Randomly select pixel indices\n",
    "    indices_to_swap = np.random.choice(len(flattened_segmentation), size=num_pixels_to_swap, replace=False)\n",
    "\n",
    "    # Assign a random class to each selected pixel\n",
    "    for idx in indices_to_swap:\n",
    "        current_class = flattened_segmentation[idx]\n",
    "        new_classes = unique_classes[unique_classes != current_class]\n",
    "        flattened_segmentation[idx] = np.random.choice(new_classes)\n",
    "\n",
    "    # Reshape back to the original shape\n",
    "    swapped_segmentation = flattened_segmentation.reshape(segmentation.shape)\n",
    "\n",
    "    return swapped_segmentation\n",
    "\n",
    "# Example usage\n",
    "label_segmentation = torch.randint(1, 5, (10, 10))\n",
    "# label_segmentation = np.array([[1, 2, 2, 3], [1, 1, 2, 3], [3, 3, 1, 2], [2, 3, 3, 1]])\n",
    "swapped_segmentation = label_swap_pixels(label_segmentation)\n",
    "print(\"Original Segmentation:\\n\", label_segmentation)\n",
    "print(\"Swapped Segmentation:\\n\", swapped_segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os.path import join\n",
    "from common.visualization import plot_segmentation\n",
    "from common.dataset import SegmentationDataset\n",
    "from common.dataset import SegmentationDataset\n",
    "from os.path import join\n",
    "from common.visualization import plot_image_label\n",
    "from common.transformations import (\n",
    "    SegCompose,\n",
    "    SegCrop,\n",
    "    SegRandomRotation,\n",
    "    SegRandomVerticalFlip,\n",
    "    SegRandomHorizontalFlip,\n",
    "    SegRandomRotation)\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "VAL_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=15, square_output = True),\n",
    "    ]\n",
    ")\n",
    "IMG_SHAPE = (256, 256)\n",
    "TRANSFORMATIONS_TORCH = Compose(\n",
    "    [\n",
    "        # CenterCrop(IMG_MAX_SIZE),\n",
    "        Resize(IMG_SHAPE),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def efficiently_swap_pixels_torch(segmentation, swap_prob=0.1):\n",
    "    # Flatten the segmentation tensor\n",
    "    flattened_segmentation = segmentation.flatten()\n",
    "\n",
    "    # Calculate the number of pixels to swap\n",
    "    num_pixels_to_swap = int(flattened_segmentation.numel() * swap_prob)\n",
    "\n",
    "    # Randomly select pixel indices\n",
    "    indices_to_swap = torch.randperm(flattened_segmentation.numel())[:num_pixels_to_swap]\n",
    "\n",
    "    # Get unique classes\n",
    "    unique_classes = torch.unique(segmentation)\n",
    "\n",
    "    # Assign a random class to each selected pixel\n",
    "    for idx in indices_to_swap:\n",
    "        current_class = flattened_segmentation[idx]\n",
    "        new_classes = unique_classes[unique_classes != current_class]\n",
    "        flattened_segmentation[idx] = new_classes[torch.randint(len(new_classes), (1,))]\n",
    "\n",
    "    # Reshape back to the original shape\n",
    "    swapped_segmentation = flattened_segmentation.reshape(segmentation.shape)\n",
    "\n",
    "    return swapped_segmentation\n",
    "\n",
    "# Example usage\n",
    "dsv = SegmentationDataset(join(\"data\", \"train_val_test\", \"train\", \"images\"), join(\"data\", \"train_val_test\", \"train\", \"segmentations\"), VAL_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "(img, label) = dsv[1]\n",
    "\n",
    "\n",
    "swapped_segmentation_torch = efficiently_swap_pixels_torch(label.clone())\n",
    "plot_segmentation(label)\n",
    "print(label.shape)\n",
    "print(label.shape)\n",
    "plot_segmentation(swapped_segmentation_torch)\n",
    "print(\"Original Segmentation:\\n\", label)\n",
    "print(\"Swapped Segmentation:\\n\", swapped_segmentation_torch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def swap_pixels_in_batch(tensor, swap_prob=0.1):\n",
    "    # Assuming tensor shape is (batch, num_classes, height, width)\n",
    "    batch_size, num_classes, height, width = tensor.shape\n",
    "\n",
    "    # Process each item in the batch separately\n",
    "    swapped_batch = []\n",
    "    for i in range(batch_size):\n",
    "        # Flatten the spatial dimensions\n",
    "        flattened = tensor[i].view(num_classes, -1)  # Shape: [num_classes, height*width]\n",
    "\n",
    "        # Calculate the number of pixels to swap\n",
    "        num_pixels_to_swap = int(height * width * swap_prob)\n",
    "\n",
    "        # Randomly select pixel indices\n",
    "        indices_to_swap = torch.randperm(height * width)[:num_pixels_to_swap]\n",
    "\n",
    "        # Swap pixels\n",
    "        for idx in indices_to_swap:\n",
    "            # Randomly choose a new class for this pixel, different from the current class\n",
    "            current_class = torch.argmax(flattened[:, idx])\n",
    "            possible_classes = [c for c in range(num_classes) if c != current_class]\n",
    "            new_class = possible_classes[torch.randint(0, len(possible_classes), (1,)).item()]\n",
    "            new_pixel = torch.eye(num_classes)[:, new_class]\n",
    "            \n",
    "            # Update the pixel in flattened tensor\n",
    "            flattened[:, idx] = new_pixel\n",
    "\n",
    "        # Reshape back to original shape and add to swapped batch\n",
    "        swapped_item = flattened.view(num_classes, height, width)\n",
    "        swapped_batch.append(swapped_item)\n",
    "\n",
    "    # Reassemble the batch\n",
    "    swapped_tensor = torch.stack(swapped_batch, dim=0)\n",
    "\n",
    "    return swapped_tensor\n",
    "\n",
    "# Example usage\n",
    "# Creating a dummy one-hot encoded segmentation (4x4 pixels, 3 classes)\n",
    "(img0, label0) = dsv[0]\n",
    "(img1, label1) = dsv[1]\n",
    "label = torch.stack((label0, label1), dim=0)\n",
    "label_oh = torch.nn.functional.one_hot(label).permute(0,3,1,2)\n",
    "# label_segmentation_one_hot_torch = torch.eye(3)[torch.randint(0, 3, (4, 4))].permute(0, 1, 2)\n",
    "swapped_segmentation_one_hot_torch = swap_pixels_in_batch(label_oh.clone())\n",
    "print(label_oh.shape)\n",
    "print(swapped_segmentation_one_hot_torch.shape)\n",
    "plot_segmentation(label_oh[0].argmax(dim=0))\n",
    "plot_segmentation(swapped_segmentation_one_hot_torch[0].argmax(dim=0))\n",
    "plot_segmentation(label_oh[1].argmax(dim=0))\n",
    "plot_segmentation(swapped_segmentation_one_hot_torch[1].argmax(dim=0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def efficiently_swap_pixels_batch_torch(segmentation_batch, swap_prob=0.1):\n",
    "    # Process each segmentation in the batch\n",
    "    swapped_batch = []\n",
    "    for segmentation in segmentation_batch:\n",
    "        # Flatten the segmentation tensor\n",
    "        flattened_segmentation = segmentation.flatten()\n",
    "\n",
    "        # Calculate the number of pixels to swap\n",
    "        num_pixels_to_swap = int(flattened_segmentation.numel() * swap_prob)\n",
    "\n",
    "        # Randomly select pixel indices\n",
    "        indices_to_swap = torch.randperm(flattened_segmentation.numel())[:num_pixels_to_swap]\n",
    "\n",
    "        # Get unique classes\n",
    "        unique_classes = torch.unique(segmentation)\n",
    "\n",
    "        # Assign a random class to each selected pixel\n",
    "        for idx in indices_to_swap:\n",
    "            current_class = flattened_segmentation[idx]\n",
    "            new_classes = unique_classes[unique_classes != current_class]\n",
    "            flattened_segmentation[idx] = new_classes[torch.randint(len(new_classes), (1,))]\n",
    "\n",
    "        # Reshape back to the original shape\n",
    "        swapped_segmentation = flattened_segmentation.reshape(segmentation.shape)\n",
    "        swapped_batch.append(swapped_segmentation)\n",
    "\n",
    "    # Stack all swapped segmentations to form a batch\n",
    "    swapped_batch_tensor = torch.stack(swapped_batch)\n",
    "\n",
    "    return swapped_batch_tensor\n",
    "# Example usage\n",
    "# Creating a dummy one-hot encoded segmentation (4x4 pixels, 3 classes)\n",
    "(img0, label0) = dsv[0]\n",
    "(img1, label1) = dsv[1]\n",
    "label = torch.stack((label0, label1), dim=0)\n",
    "# label_segmentation_one_hot_torch = torch.eye(3)[torch.randint(0, 3, (4, 4))].permute(0, 1, 2)\n",
    "swapped_segmentation_torch = efficiently_swap_pixels_batch_torch(label.clone())\n",
    "print(label.shape)\n",
    "print(swapped_segmentation_torch.shape)\n",
    "plot_segmentation(label[0])\n",
    "plot_segmentation(swapped_segmentation_torch[0])\n",
    "plot_segmentation(label[1])\n",
    "plot_segmentation(swapped_segmentation_torch[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_oh = torch.nn.functional.one_hot(label).permute(2,0,1)\n",
    "print(label_oh.argmax(dim=0))\n",
    "plot_segmentation(label_oh.argmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.dataset import SegmentationDataset\n",
    "from os.path import join\n",
    "from common.visualization import plot_image_label\n",
    "from common.transformations import (\n",
    "    SegCompose,\n",
    "    SegCrop,\n",
    "    SegRandomRotation,\n",
    "    SegRandomVerticalFlip,\n",
    "    SegRandomHorizontalFlip,\n",
    "    SegRandomRotation,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    CenterCrop,\n",
    ")\n",
    "IMG_MAX_SIZE = (380,380)\n",
    "TRAIN_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        # SegRandomRotation(0.99),\n",
    "        SegCrop(default_t=15, ),\n",
    "    ]\n",
    ")\n",
    "VAL_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=15, square_output = True),\n",
    "    ]\n",
    ")\n",
    "IMG_SHAPE = (256, 256)\n",
    "TRANSFORMATIONS_TORCH = Compose(\n",
    "    [\n",
    "        # CenterCrop(IMG_MAX_SIZE),\n",
    "        Resize(IMG_SHAPE),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "VAL_TRANSFORMATIONS_SEG2 = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=15, default_size=IMG_MAX_SIZE),\n",
    "    ]\n",
    ")\n",
    "dst = SegmentationDataset(join(\"data\", \"train_val_test\", \"train\", \"images\"), join(\"data\", \"train_val_test\", \"train\", \"segmentations\"), TRAIN_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "for i in range(min(20,len(dsv))):\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    (img, label) = dst[i]\n",
    "    plot_image_label(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.dataset import SegmentationDataset\n",
    "from os.path import join\n",
    "from common.visualization import plot_image_label\n",
    "from common.transformations import (\n",
    "    SegCompose,\n",
    "    SegCrop,\n",
    "    SegRandomRotation,\n",
    "    SegRandomVerticalFlip,\n",
    "    SegRandomHorizontalFlip,\n",
    "    SegRandomRotation,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    CenterCrop,\n",
    ")\n",
    "IMG_MAX_SIZE = (380,380)\n",
    "TRAIN_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        SegRandomRotation(),\n",
    "        SegRandomVerticalFlip(),\n",
    "        SegRandomHorizontalFlip(),\n",
    "        # SegCrop(default_t=15, default_size=IMG_MAX_SIZE),\n",
    "    ]\n",
    ")\n",
    "VAL_TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        # SegCrop(default_t=15, square_output = True),\n",
    "    ]\n",
    ")\n",
    "IMG_SHAPE = (256, 256)\n",
    "TRANSFORMATIONS_TORCH = Compose(\n",
    "    [\n",
    "        # CenterCrop(IMG_MAX_SIZE),\n",
    "        Resize(IMG_SHAPE),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "VAL_TRANSFORMATIONS_SEG2 = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=15, default_size=IMG_MAX_SIZE),\n",
    "    ]\n",
    ")\n",
    "dst = SegmentationDataset(join(\"data\", \"weak_annotations\", \"images\"), join(\"data\", \"weak_annotations\", \"segmentations\"), TRAIN_TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH)\n",
    "for i in range(min(20,len(dst))):\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    (img, label) = dst[i]\n",
    "    plot_image_label(img, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
