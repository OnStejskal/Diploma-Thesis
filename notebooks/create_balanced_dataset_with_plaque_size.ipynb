{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from os.path import join\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNT CARDINALITIES FOR CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_PATH = os.path.join(\"data\", \"forecast_key_dataset.csv\")\n",
    "df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "echos = [(df['echogenicity'] == 1).sum(), (df['echogenicity'] == 2).sum(), (df['echogenicity'] == 3).sum(), (df['echogenicity'] == 4).sum(), ]\n",
    "homos = [(df['homogenicity'] == 1).sum(), (df['homogenicity'] == 2).sum()] \n",
    "print(echos)\n",
    "class_cardinalities = np.array(echos)\n",
    "n_classes = len(class_cardinalities)\n",
    "N = class_cardinalities.sum()\n",
    "\n",
    "weights = N / (n_classes * class_cardinalities)\n",
    "print(weights)\n",
    "\n",
    "\n",
    "print(homos)\n",
    "class_cardinalities = np.array(homos)\n",
    "n_classes = len(class_cardinalities)\n",
    "N = class_cardinalities.sum()\n",
    "\n",
    "weights = N / (n_classes * class_cardinalities)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA = os.path.join(\"data\", \"segmentation_data\", \"data\", \"trans\")\n",
    "INPUT_SEG = os.path.join(\"data\", \"segmentation_data\", \"references\", \"trans\")\n",
    "\n",
    "LABEL_PATH = os.path.join(\"parameters_computation\", \"data\", \"data_with_attributes.csv\")\n",
    "\n",
    "def count_green_pixels(image_path):\n",
    "    # Load the image\n",
    "    green_range = ([0, 100, 0], [150, 255, 150]) \n",
    "    image = Image.open(image_path)\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Define the range for green pixels\n",
    "    lower_green, upper_green = green_range\n",
    "    # Count green pixels\n",
    "    green_pixels = np.all(np.logical_and(lower_green <= image_np, image_np <= upper_green), axis=-1)\n",
    "    count = np.sum(green_pixels)\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def create_balanced_homogenicity(path_img, path_seg):\n",
    "\n",
    "    files_img = os.listdir(path_img)\n",
    "    files_seg_unprocessed = os.listdir(path_seg)\n",
    "    print(f'files segmentation all: {len(files_seg_unprocessed)}')\n",
    "    files_seg = []\n",
    "    for f in files_seg_unprocessed:\n",
    "        image_path = join(path_seg, f)\n",
    "        if count_green_pixels(image_path) > 1000:\n",
    "            files_seg.append(f)\n",
    "    print(f'files plaque big enopugh all: {len(files_seg)}')\n",
    "\n",
    "    labels_df = pd.read_csv(LABEL_PATH)\n",
    "    labels_df.drop_duplicates(subset=['img_name'], keep=False, inplace=True)\n",
    "    files_labels_class1 = []\n",
    "    files_labels_class2 = []\n",
    "    for i, row in labels_df.iterrows():\n",
    "        if pd.notna(row[\"echogenicity\"]) and pd.notna(row[\"homogenicity\"]):\n",
    "            if row[\"homogenicity\"] == 1:\n",
    "                files_labels_class1.append(row[\"img_name\"])\n",
    "            elif row[\"homogenicity\"] == 2:\n",
    "                files_labels_class2.append(row[\"img_name\"])\n",
    "            else:\n",
    "                print(row[\"homogenicity\"])\n",
    "    \n",
    "    class1 = list(set(files_img) & set(files_seg) & set(files_labels_class1))\n",
    "    class2 = list(set(files_img) & set(files_seg) & set(files_labels_class2))\n",
    "\n",
    "    # class1 = []\n",
    "    # class2 = []\n",
    "    # for item in common_items:\n",
    "\n",
    "\n",
    "    return class1, class2\n",
    "\n",
    "input_experiment_path = \"balanced_annotated\"\n",
    "c1, c2 = create_balanced_homogenicity(INPUT_DATA,INPUT_SEG)\n",
    "random.shuffle(c1)\n",
    "random.shuffle(c2)\n",
    "min_len = min(len(c1), len(c2))\n",
    "c1 = c1[:min_len]\n",
    "c2 = c2[:min_len]\n",
    "print(len(c1), len(c2))\n",
    "# image_files = c1 + c2\n",
    "random.shuffle(c1)\n",
    "random.shuffle(c2)\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "num_train = int(train_ratio * len(c1))\n",
    "num_val = int((train_ratio+val_ratio) * len(c1))\n",
    "\n",
    "train_set = c1[:num_train] + c2[:num_train]\n",
    "val_set = c1[num_train:num_val] + c2[num_train:num_val]\n",
    "test_set = c1[num_val:] + c2[num_val:]\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "\n",
    "\n",
    "images_folder = INPUT_DATA\n",
    "labels_folder = INPUT_SEG\n",
    "\n",
    "dir_path = os.path.join(\"parameters_computation\", \"data\", input_experiment_path)\n",
    "\n",
    "# Create train and validation folders if not exists\n",
    "os.makedirs(dir_path)\n",
    "\n",
    "os.makedirs(join(dir_path, \"train\"))\n",
    "os.makedirs(join(dir_path, \"val\"))\n",
    "\n",
    "train_images_path =join(dir_path, \"train\", \"images\")\n",
    "val_images_path = join(dir_path, \"val\",\"images\")\n",
    "train_segmentaion_path = join(dir_path, \"train\", \"segmentations\")\n",
    "val_segmentations_path = join(dir_path, \"val\", \"segmentations\")\n",
    "\n",
    "test_images_path =join(dir_path, \"test\", \"images\")\n",
    "test_segmentations_path =join(dir_path, \"test\", \"segmentations\")\n",
    "\n",
    "\n",
    "os.makedirs(train_images_path)\n",
    "os.makedirs(val_images_path)\n",
    "os.makedirs(train_segmentaion_path)\n",
    "os.makedirs(val_segmentations_path)\n",
    "os.makedirs(test_images_path)\n",
    "os.makedirs(test_segmentations_path)\n",
    "\n",
    "# Copy images to train folder\n",
    "for image_file in train_set:\n",
    "    src_image = os.path.join(images_folder, image_file)\n",
    "    dst_image = os.path.join(train_images_path, image_file)\n",
    "    shutil.copy(src_image, dst_image)\n",
    "\n",
    "# Copy labels to train folder\n",
    "for label_file in train_set:\n",
    "    src_label = os.path.join(labels_folder, label_file)\n",
    "    dst_label = os.path.join(train_segmentaion_path, label_file)\n",
    "    shutil.copy(src_label, dst_label)\n",
    "\n",
    "# Copy remaining images to validation folder\n",
    "for image_file in val_set:\n",
    "    src_image = os.path.join(images_folder, image_file)\n",
    "    dst_image = os.path.join(val_images_path, image_file)\n",
    "    shutil.copy(src_image, dst_image)\n",
    "\n",
    "# Copy remaining labels to validation folder\n",
    "for label_file in val_set:\n",
    "    src_label = os.path.join(labels_folder, label_file)\n",
    "    dst_label = os.path.join(val_segmentations_path, label_file)\n",
    "    shutil.copy(src_label, dst_label)\n",
    "\n",
    "\n",
    "# Copy remaining images to validation folder\n",
    "for image_file in test_set:\n",
    "    src_image = os.path.join(images_folder, image_file)\n",
    "    dst_image = os.path.join(test_images_path, image_file)\n",
    "    shutil.copy(src_image, dst_image)\n",
    "\n",
    "# Copy remaining labels to validation folder\n",
    "for label_file in test_set:\n",
    "    src_label = os.path.join(labels_folder, label_file)\n",
    "    dst_label = os.path.join(test_segmentations_path, label_file)\n",
    "    shutil.copy(src_label, dst_label)\n",
    "\n",
    "train_folder = join(dir_path, \"train\", \"images\")\n",
    "test_folder = join(dir_path, \"test\", \"images\")\n",
    "val_folder = join(dir_path, \"val\", \"images\")\n",
    "\n",
    "train_files = os.listdir(train_folder)\n",
    "val_files = os.listdir(val_folder)\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))\n",
    "\n",
    "df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "count1 = len(df[(df['homogenicity'] == 1) & (df['img_name'].isin(train_files))])\n",
    "count2 = len(df[(df['homogenicity'] == 2) & (df['img_name'].isin(train_files))])\n",
    "print(f\"train {count1}, {count2}\")\n",
    "\n",
    "count1 = len(df[(df['homogenicity'] == 1) & (df['img_name'].isin(test_files))])\n",
    "count2 = len(df[(df['homogenicity'] == 2) & (df['img_name'].isin(test_files))])\n",
    "print(f\"test_files {count1}, {count2}\")\n",
    "\n",
    "count1 = len(df[(df['homogenicity'] == 1) & (df['img_name'].isin(val_files))])\n",
    "count2 = len(df[(df['homogenicity'] == 2) & (df['img_name'].isin(val_files))])\n",
    "print(f\"val_files {count1}, {count2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_echogenicity(path_img, path_seg):\n",
    "\n",
    "    files_img = os.listdir(path_img)\n",
    "    files_seg = os.listdir(path_seg)\n",
    "\n",
    "    labels_df = pd.read_csv(os.path.join(\"parameters_computation\", \"data\", \"forecast_key_dataset.csv\"))\n",
    "    labels_df.drop_duplicates(subset=['img_name'], keep=False, inplace=True)\n",
    "    files_labels_class1 = []\n",
    "    files_labels_class2 = []\n",
    "    files_labels_class3 = []\n",
    "    files_labels_class4 = []\n",
    "    for i, row in labels_df.iterrows():\n",
    "        if pd.notna(row[\"echogenicity\"]) and pd.notna(row[\"homogenicity\"]):\n",
    "            if row[\"echogenicity\"] == 1:\n",
    "                files_labels_class1.append(row[\"img_name\"])\n",
    "            elif row[\"echogenicity\"] == 2:\n",
    "                files_labels_class2.append(row[\"img_name\"])\n",
    "            elif row[\"echogenicity\"] == 3:\n",
    "                files_labels_class3.append(row[\"img_name\"])\n",
    "            elif row[\"echogenicity\"] == 4:\n",
    "                files_labels_class4.append(row[\"img_name\"])\n",
    "            else:\n",
    "                print(row[\"homogenicity\"])\n",
    "    \n",
    "    class1 = list(set(files_img) & set(files_seg) & set(files_labels_class1))\n",
    "    class2 = list(set(files_img) & set(files_seg) & set(files_labels_class2))\n",
    "    class3 = list(set(files_img) & set(files_seg) & set(files_labels_class3))\n",
    "    class4 = list(set(files_img) & set(files_seg) & set(files_labels_class4))\n",
    "\n",
    "    print(len(class1),len(class2),len(class3), len(class4))\n",
    "    # class1 = []\n",
    "    # class2 = []\n",
    "    # for item in common_items:\n",
    "\n",
    "\n",
    "    return class1, class2, class3, class4\n",
    "\n",
    "c1, c2, c3, c4 = create_balanced_echogenicity(os.path.join(\"data\",\"carotid_key_inpainted_dataset\",\"UNET_best_model_segmentation\",\"localized\"),os.path.join(\"data\",\"carotid_key_inpainted_dataset\",\"UNET_best_model_segmentation\",\"segmentations\"))\n",
    "random.shuffle(c1)\n",
    "random.shuffle(c2)\n",
    "random.shuffle(c3)\n",
    "random.shuffle(c4)\n",
    "min_len = min(len(c1), len(c2), len(c3), len(c4))\n",
    "c1 = c1[:min_len]\n",
    "c2 = c2[:min_len]\n",
    "c3 = c3[:min_len]\n",
    "c4 = c4[:min_len]\n",
    "\n",
    "print(len(c1), len(c2), len(c3), len(c4))\n",
    "# image_files = c1 + c2\n",
    "random.shuffle(c1)\n",
    "random.shuffle(c2)\n",
    "random.shuffle(c3)\n",
    "random.shuffle(c4)\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "num_train = int(train_ratio * len(c1))\n",
    "num_val = int((train_ratio+val_ratio) * len(c1))\n",
    "\n",
    "train_set = c1[:num_train] + c2[:num_train] + c3[:num_train] + c4[:num_train]\n",
    "val_set = c1[num_train:num_val] + c2[num_train:num_val] + c3[num_train:num_val] + c4[num_train:num_val]\n",
    "test_set = c1[num_val:] + c2[num_val:] + c3[num_val:] + c4[num_val:]\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "\n",
    "\n",
    "images_folder = os.path.join(\"data\",\"carotid_key_inpainted_dataset\",\"UNET_best_model_segmentation\",\"localized\")\n",
    "labels_folder = os.path.join(\"data\",\"carotid_key_inpainted_dataset\",\"UNET_best_model_segmentation\",\"segmentations\")\n",
    "dir_path = os.path.join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenicity_labels\")\n",
    "\n",
    "\n",
    "\n",
    "# Create train and validation folders if not exists\n",
    "os.makedirs(dir_path)\n",
    "\n",
    "os.makedirs(join(dir_path, \"train\"))\n",
    "os.makedirs(join(dir_path, \"val\"))\n",
    "\n",
    "train_images_path =join(dir_path, \"train\", \"images\")\n",
    "val_images_path = join(dir_path, \"val\",\"images\")\n",
    "train_segmentaion_path = join(dir_path, \"train\", \"segmentations\")\n",
    "val_segmentations_path = join(dir_path, \"val\", \"segmentations\")\n",
    "\n",
    "test_images_path =join(dir_path, \"test\", \"images\")\n",
    "test_segmentations_path =join(dir_path, \"test\", \"segmentations\")\n",
    "\n",
    "\n",
    "os.makedirs(train_images_path)\n",
    "os.makedirs(val_images_path)\n",
    "os.makedirs(train_segmentaion_path)\n",
    "os.makedirs(val_segmentations_path)\n",
    "os.makedirs(test_images_path)\n",
    "os.makedirs(test_segmentations_path)\n",
    "\n",
    "# Copy images to train folder\n",
    "for image_file in train_set:\n",
    "    src_image = os.path.join(images_folder, image_file)\n",
    "    dst_image = os.path.join(train_images_path, image_file)\n",
    "    shutil.copy(src_image, dst_image)\n",
    "\n",
    "# Copy labels to train folder\n",
    "for label_file in train_set:\n",
    "    src_label = os.path.join(labels_folder, label_file)\n",
    "    dst_label = os.path.join(train_segmentaion_path, label_file)\n",
    "    shutil.copy(src_label, dst_label)\n",
    "\n",
    "# Copy remaining images to validation folder\n",
    "for image_file in val_set:\n",
    "    src_image = os.path.join(images_folder, image_file)\n",
    "    dst_image = os.path.join(val_images_path, image_file)\n",
    "    shutil.copy(src_image, dst_image)\n",
    "\n",
    "# Copy remaining labels to validation folder\n",
    "for label_file in val_set:\n",
    "    src_label = os.path.join(labels_folder, label_file)\n",
    "    dst_label = os.path.join(val_segmentations_path, label_file)\n",
    "    shutil.copy(src_label, dst_label)\n",
    "\n",
    "\n",
    "# Copy remaining images to validation folder\n",
    "for image_file in test_set:\n",
    "    src_image = os.path.join(images_folder, image_file)\n",
    "    dst_image = os.path.join(test_images_path, image_file)\n",
    "    shutil.copy(src_image, dst_image)\n",
    "\n",
    "# Copy remaining labels to validation folder\n",
    "for label_file in test_set:\n",
    "    src_label = os.path.join(labels_folder, label_file)\n",
    "    dst_label = os.path.join(test_segmentations_path, label_file)\n",
    "    shutil.copy(src_label, dst_label)\n",
    "\n",
    "train_folder = join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenecity_labels\", \"train\", \"images\")\n",
    "test_folder = join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenecity_labels\", \"test\", \"images\")\n",
    "val_folder = join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenecity_labels\", \"val\", \"images\")\n",
    "\n",
    "train_files = os.listdir(train_folder)\n",
    "val_files = os.listdir(val_folder)\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"parameters_computation\", \"data\", \"forecast_key_dataset.csv\"))\n",
    "\n",
    "count1 = len(df[(df['echogenicity'] == 1) & (df['img_name'].isin(train_files))])\n",
    "count2 = len(df[(df['echogenicity'] == 2) & (df['img_name'].isin(train_files))])\n",
    "count3 = len(df[(df['echogenicity'] == 3) & (df['img_name'].isin(train_files))])\n",
    "count4 = len(df[(df['echogenicity'] == 4) & (df['img_name'].isin(train_files))])\n",
    "print(f\"train {count1}, {count2}, {count3}, {count4}\")\n",
    "\n",
    "count1 = len(df[(df['echogenicity'] == 1) & (df['img_name'].isin(test_files))])\n",
    "count2 = len(df[(df['echogenicity'] == 2) & (df['img_name'].isin(test_files))])\n",
    "count3 = len(df[(df['echogenicity'] == 3) & (df['img_name'].isin(test_files))])\n",
    "count4 = len(df[(df['echogenicity'] == 4) & (df['img_name'].isin(test_files))])\n",
    "print(f\"test_files {count1}, {count2}, {count3}, {count4}\")\n",
    "\n",
    "count1 = len(df[(df['echogenicity'] == 1) & (df['img_name'].isin(val_files))])\n",
    "count2 = len(df[(df['echogenicity'] == 2) & (df['img_name'].isin(val_files))])\n",
    "count3 = len(df[(df['echogenicity'] == 3) & (df['img_name'].isin(val_files))])\n",
    "count4 = len(df[(df['echogenicity'] == 4) & (df['img_name'].isin(val_files))])\n",
    "print(f\"val_files {count1}, {count2}, {count3}, {count4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenicity_labels\", \"train\", \"images\")\n",
    "test_folder = join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenicity_labels\", \"test\", \"images\")\n",
    "val_folder = join(\"parameters_computation\", \"data\", \"best_unet_balanced_echogenicity_labels\", \"val\", \"images\")\n",
    "\n",
    "train_files = os.listdir(train_folder)\n",
    "val_files = os.listdir(val_folder)\n",
    "test_files = os.listdir(test_folder)\n",
    "\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "print(len(test_files))\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"parameters_computation\", \"data\", \"forecast_key_dataset.csv\"))\n",
    "\n",
    "count1 = len(df[(df['echogenicity'] == 1) & (df['img_name'].isin(train_files))])\n",
    "count2 = len(df[(df['echogenicity'] == 2) & (df['img_name'].isin(train_files))])\n",
    "count3 = len(df[(df['echogenicity'] == 3) & (df['img_name'].isin(train_files))])\n",
    "count4 = len(df[(df['echogenicity'] == 4) & (df['img_name'].isin(train_files))])\n",
    "print(f\"train {count1}, {count2}, {count3}, {count4}\")\n",
    "\n",
    "count1 = len(df[(df['echogenicity'] == 1) & (df['img_name'].isin(test_files))])\n",
    "count2 = len(df[(df['echogenicity'] == 2) & (df['img_name'].isin(test_files))])\n",
    "count3 = len(df[(df['echogenicity'] == 3) & (df['img_name'].isin(test_files))])\n",
    "count4 = len(df[(df['echogenicity'] == 4) & (df['img_name'].isin(test_files))])\n",
    "print(f\"test_files {count1}, {count2}, {count3}, {count4}\")\n",
    "\n",
    "count1 = len(df[(df['echogenicity'] == 1) & (df['img_name'].isin(val_files))])\n",
    "count2 = len(df[(df['echogenicity'] == 2) & (df['img_name'].isin(val_files))])\n",
    "count3 = len(df[(df['echogenicity'] == 3) & (df['img_name'].isin(val_files))])\n",
    "count4 = len(df[(df['echogenicity'] == 4) & (df['img_name'].isin(val_files))])\n",
    "print(f\"val_files {count1}, {count2}, {count3}, {count4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "def count_green_pixels(image_path, green_range):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Define the range for green pixels\n",
    "    lower_green, upper_green = green_range\n",
    "\n",
    "    # Count green pixels\n",
    "    green_pixels = np.all(np.logical_and(lower_green <= image_np, image_np <= upper_green), axis=-1)\n",
    "    count = np.sum(green_pixels)\n",
    "\n",
    "    return count\n",
    "\n",
    "# Example usage\n",
    "green_range = ([0, 100, 0], [150, 255, 150])  # Adjust these values based on your definition of green\n",
    "\n",
    "count = 0\n",
    "count2 = 0\n",
    "seg_path = \"data//carotid_key_inpainted_dataset//localization9_UNET_best_model_segmentation_trainvaltest//segmentations\"\n",
    "files = listdir(seg_path)\n",
    "print(len(files))\n",
    "for f in files:\n",
    "    image_path = join(seg_path, f)\n",
    "    if count_green_pixels(image_path, green_range) > 1000:\n",
    "        count +=1\n",
    "    else:\n",
    "        count2 += 1\n",
    "\n",
    "print(count)\n",
    "print(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'images' is a list of 3D tensors (channel, width, height)\n",
    "# Example: images = [np.random.rand(3, 100, 100), np.random.rand(3, 100, 100)]\n",
    "\n",
    "def non_zero_pixels(image):\n",
    "    # Filter out pixels where any channel is zero\n",
    "    mask = np.all(image > 0, axis=0)\n",
    "    return image[:, mask]\n",
    "\n",
    "def mean_intensity_values(filtered_image):\n",
    "    # Calculate mean intensity for each channel\n",
    "    return np.mean(filtered_image, axis=(1, 2))\n",
    "\n",
    "# Collect mean intensity values for each channel\n",
    "channel_means = {0: [], 1: [], 2: []}\n",
    "\n",
    "for image in images:\n",
    "    filtered_image = non_zero_pixels(image)\n",
    "    means = mean_intensity_values(filtered_image)\n",
    "    for i, mean in enumerate(means):\n",
    "        channel_means[i].append(mean)\n",
    "\n",
    "# Create boxplots\n",
    "plt.boxplot([channel_means[0], channel_means[1], channel_means[2]], labels=['Channel 1', 'Channel 2', 'Channel 3'])\n",
    "plt.title('Boxplot of Mean Intensity Values for Each Channel')\n",
    "plt.ylabel('Mean Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
