{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import sum, mean\n",
    "import torch\n",
    "from common.dataset import SegmentationDataset\n",
    "from common.transformations import (\n",
    "    SegCompose,\n",
    "    SegCrop,\n",
    "    SegRandomHorizontalFlip,\n",
    "    SegRandomVerticalFlip,\n",
    ")\n",
    "from os.path import join\n",
    "from torch import cuda, device, save\n",
    "from os.path import join\n",
    "from torch.nn import CrossEntropyLoss, Module\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    "    Resize,\n",
    "    ToTensor,)\n",
    "\n",
    "\n",
    "TRANSVERSE_TRAIN_IMG_PATH = join(\"data\", \"seg_train_small\", \"imgs\", \"trans\")\n",
    "TRANSVERSE_TRAIN_LABELS_PATH = join(\"data\", \"seg_train_small\", \"labels\", \"trans\")\n",
    "# transformations\n",
    "TRANSFORMATIONS_SEG = SegCompose(\n",
    "    [\n",
    "        SegCrop(default_t=15),\n",
    "    ]\n",
    ")\n",
    "TRANSFORMATIONS_TORCH = Compose(\n",
    "    [\n",
    "        # original| Resize((512, 512)),\n",
    "        Resize((128, 128)),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "val_dataset = SegmentationDataset(\n",
    "        TRANSVERSE_TRAIN_IMG_PATH, TRANSVERSE_TRAIN_LABELS_PATH, TRANSFORMATIONS_SEG, TRANSFORMATIONS_TORCH\n",
    "    )\n",
    "\n",
    "img = val_dataset[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1608, 0.1647, 0.1608,  ..., 0.1647, 0.1725, 0.1725],\n",
       "         [0.1725, 0.1608, 0.1451,  ..., 0.1765, 0.1922, 0.1922],\n",
       "         [0.1725, 0.1686, 0.1451,  ..., 0.1882, 0.1882, 0.2000],\n",
       "         ...,\n",
       "         [0.0941, 0.1059, 0.1059,  ..., 0.0588, 0.0510, 0.0392],\n",
       "         [0.0941, 0.0980, 0.0902,  ..., 0.0431, 0.0431, 0.0314],\n",
       "         [0.0941, 0.0902, 0.0941,  ..., 0.0235, 0.0196, 0.0157]],\n",
       "\n",
       "        [[0.1686, 0.1686, 0.1686,  ..., 0.1725, 0.1765, 0.1765],\n",
       "         [0.1765, 0.1647, 0.1529,  ..., 0.1843, 0.1961, 0.1961],\n",
       "         [0.1804, 0.1765, 0.1490,  ..., 0.1961, 0.1961, 0.2078],\n",
       "         ...,\n",
       "         [0.0980, 0.1098, 0.1098,  ..., 0.0588, 0.0549, 0.0431],\n",
       "         [0.0980, 0.1020, 0.0941,  ..., 0.0431, 0.0431, 0.0353],\n",
       "         [0.0980, 0.0941, 0.0980,  ..., 0.0235, 0.0196, 0.0157]],\n",
       "\n",
       "        [[0.1725, 0.1765, 0.1765,  ..., 0.1765, 0.1843, 0.1843],\n",
       "         [0.1843, 0.1725, 0.1608,  ..., 0.1922, 0.2039, 0.2039],\n",
       "         [0.1882, 0.1843, 0.1569,  ..., 0.2039, 0.2000, 0.2157],\n",
       "         ...,\n",
       "         [0.0980, 0.1137, 0.1137,  ..., 0.0627, 0.0549, 0.0471],\n",
       "         [0.1020, 0.1059, 0.0980,  ..., 0.0431, 0.0471, 0.0353],\n",
       "         [0.1020, 0.0980, 0.1020,  ..., 0.0235, 0.0196, 0.0157]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1567, 0.1637, 0.1709])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_mean = torch.std(val_dataset[0][0], dim=(1,2))\n",
    "# img.shape\n",
    "img_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.max(val_dataset[0][0], dim=(1,2))\n",
    "torch.min(val_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4158, -1.3987, -1.4158,  ..., -1.3987, -1.3644, -1.3644],\n",
       "         [-1.3644, -1.4158, -1.4843,  ..., -1.3473, -1.2788, -1.2788],\n",
       "         [-1.3644, -1.3815, -1.4843,  ..., -1.2959, -1.2959, -1.2445],\n",
       "         ...,\n",
       "         [-1.7069, -1.6555, -1.6555,  ..., -1.8610, -1.8953, -1.9467],\n",
       "         [-1.7069, -1.6898, -1.7240,  ..., -1.9295, -1.9295, -1.9809],\n",
       "         [-1.7069, -1.7240, -1.7069,  ..., -2.0152, -2.0323, -2.0494]],\n",
       "\n",
       "        [[-1.2829, -1.2829, -1.2829,  ..., -1.2654, -1.2479, -1.2479],\n",
       "         [-1.2479, -1.3004, -1.3529,  ..., -1.2129, -1.1604, -1.1604],\n",
       "         [-1.2304, -1.2479, -1.3704,  ..., -1.1604, -1.1604, -1.1078],\n",
       "         ...,\n",
       "         [-1.5980, -1.5455, -1.5455,  ..., -1.7731, -1.7906, -1.8431],\n",
       "         [-1.5980, -1.5805, -1.6155,  ..., -1.8431, -1.8431, -1.8782],\n",
       "         [-1.5980, -1.6155, -1.5980,  ..., -1.9307, -1.9482, -1.9657]],\n",
       "\n",
       "        [[-1.0376, -1.0201, -1.0201,  ..., -1.0201, -0.9853, -0.9853],\n",
       "         [-0.9853, -1.0376, -1.0898,  ..., -0.9504, -0.8981, -0.8981],\n",
       "         [-0.9678, -0.9853, -1.1073,  ..., -0.8981, -0.9156, -0.8458],\n",
       "         ...,\n",
       "         [-1.3687, -1.2990, -1.2990,  ..., -1.5256, -1.5604, -1.5953],\n",
       "         [-1.3513, -1.3339, -1.3687,  ..., -1.6127, -1.5953, -1.6476],\n",
       "         [-1.3513, -1.3687, -1.3513,  ..., -1.6999, -1.7173, -1.7347]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define mean and std deviation values\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "# Standardize the image\n",
    "standardized_image = (img - mean[:, None, None]) / std[:, None, None]\n",
    "standardized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4158, -1.3987, -1.4158,  ..., -1.3987, -1.3644, -1.3644],\n",
       "         [-1.3644, -1.4158, -1.4843,  ..., -1.3473, -1.2788, -1.2788],\n",
       "         [-1.3644, -1.3815, -1.4843,  ..., -1.2959, -1.2959, -1.2445],\n",
       "         ...,\n",
       "         [-1.7069, -1.6555, -1.6555,  ..., -1.8610, -1.8953, -1.9467],\n",
       "         [-1.7069, -1.6898, -1.7240,  ..., -1.9295, -1.9295, -1.9809],\n",
       "         [-1.7069, -1.7240, -1.7069,  ..., -2.0152, -2.0323, -2.0494]],\n",
       "\n",
       "        [[-1.2829, -1.2829, -1.2829,  ..., -1.2654, -1.2479, -1.2479],\n",
       "         [-1.2479, -1.3004, -1.3529,  ..., -1.2129, -1.1604, -1.1604],\n",
       "         [-1.2304, -1.2479, -1.3704,  ..., -1.1604, -1.1604, -1.1078],\n",
       "         ...,\n",
       "         [-1.5980, -1.5455, -1.5455,  ..., -1.7731, -1.7906, -1.8431],\n",
       "         [-1.5980, -1.5805, -1.6155,  ..., -1.8431, -1.8431, -1.8782],\n",
       "         [-1.5980, -1.6155, -1.5980,  ..., -1.9307, -1.9482, -1.9657]],\n",
       "\n",
       "        [[-1.0376, -1.0201, -1.0201,  ..., -1.0201, -0.9853, -0.9853],\n",
       "         [-0.9853, -1.0376, -1.0898,  ..., -0.9504, -0.8981, -0.8981],\n",
       "         [-0.9678, -0.9853, -1.1073,  ..., -0.8981, -0.9156, -0.8458],\n",
       "         ...,\n",
       "         [-1.3687, -1.2990, -1.2990,  ..., -1.5256, -1.5604, -1.5953],\n",
       "         [-1.3513, -1.3339, -1.3687,  ..., -1.6127, -1.5953, -1.6476],\n",
       "         [-1.3513, -1.3687, -1.3513,  ..., -1.6999, -1.7173, -1.7347]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "# Standardize the image tensor to have ImageNet mean and std\n",
    "standardized_image = (img - imagenet_mean[:, None, None]) / imagenet_std[:, None, None]\n",
    "standardized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4490, -1.3232, -1.0668])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(standardized_image, dim = (1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
